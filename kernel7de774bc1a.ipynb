{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/google-quest-challenge/sample_submission.csv\n",
      "/kaggle/input/google-quest-challenge/test.csv\n",
      "/kaggle/input/google-quest-challenge/train.csv\n",
      "/kaggle/input/xlnet-base-tf/xlnet-base-cased-config.json\n",
      "/kaggle/input/xlnet-base-tf/xlnet-base-cased-tf_model.h5\n",
      "/kaggle/input/xlnet-base-tf/xlnet-base-cased-spiece.model\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "# !pip install /kaggle/input/kerasselfattention/keras-self-attention-0.42.0\n",
    "# import tensorflow as tf\n",
    "import os\n",
    "# from tensorflow.keras.layers import Layer\n",
    "# import tensorflow.keras.backend as K\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import tensorflow_hub as hub\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# import config\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import nltk\n",
    "# import keras.backend as K\n",
    "# from nltk.probability import FreqDist\n",
    "# from nltk.corpus import stopwords\n",
    "# import string\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# # nltk.download('stopwords')\n",
    "# # nltk.download('punkt')\n",
    "# eng_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "# import gc, os, pickle\n",
    "# from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def plot_len(df, col_name, i):\n",
    "    plt.figure(i)\n",
    "    sns.distplot(df[col_name].str.len())\n",
    "    plt.ylabel(\"length of string\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_cnt_words(df, col_name, i):\n",
    "    plt.figure(i)\n",
    "    vals = df[col_name].apply(lambda x: len(x.strip().split()))\n",
    "    sns.distplot(vals)\n",
    "    plt.ylabel(\"count of words\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"havent\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"thats\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"theres\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"theyre\":  \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]    \n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def clean_data(df, columns: list):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n",
    "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "def plot_freq_dist(train_data):\n",
    "    freq_dist = FreqDist([word for text in train_data['question_body'].str.replace('[^a-za-z0-9^,!.\\/+-=]',' ') for word in text.split()])\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    plt.title('Word frequency on question title (Training Data)').set_fontsize(25)\n",
    "    plt.xlabel('').set_fontsize(25)\n",
    "    plt.ylabel('').set_fontsize(25)\n",
    "    freq_dist.plot(60,cumulative=False)\n",
    "    plt.show()\n",
    "\n",
    "def get_tfidf_features(data, dims=256):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    tsvd = TruncatedSVD(n_components = dims, n_iter=5)\n",
    "    tfquestion_title = tfidf.fit_transform(data[\"question_title\"].values)\n",
    "    tfquestion_title = tsvd.fit_transform(tfquestion_title)\n",
    "\n",
    "    tfquestion_body = tfidf.fit_transform(data[\"question_body\"].values)\n",
    "    tfquestion_body = tsvd.fit_transform(tfquestion_body)\n",
    "\n",
    "    tfanswer = tfidf.fit_transform(data[\"answer\"].values)\n",
    "    tfanswer = tsvd.fit_transform(tfanswer)\n",
    "\n",
    "    return tfquestion_title, tfquestion_body, tfanswer\n",
    "\n",
    "def correlation(x, y):    \n",
    "    mx = tf.math.reduce_mean(x)\n",
    "    my = tf.math.reduce_mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = tf.math.reduce_mean(tf.multiply(xm,ym))        \n",
    "    r_den = tf.math.reduce_std(xm) * tf.math.reduce_std(ym)\n",
    "    return  r_num / r_den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, Concatenate, GRU, Maximum\n",
    "# from keras.models import Model\n",
    "# from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "tokens = []\n",
    "def get_words(col):\n",
    "  global tokens\n",
    "  toks = []\n",
    "  for x in sent_tokenize(col):\n",
    "    tokens += word_tokenize(x)\n",
    "    toks += word_tokenize(x)\n",
    "  return toks\n",
    "\n",
    "def convert_to_indx(col, word2idx, vocab_size):\n",
    "  return [word2idx[word] if word in word2idx else vocab_size for word in col]\n",
    "\n",
    "def LSTM_model_initial(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n",
    "                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n",
    "                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n",
    "    columns = ['question_title','question_body','answer']\n",
    "    df_train = clean_data(df_train, columns)\n",
    "    df_test = clean_data(df_test, columns)\n",
    "    # columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n",
    "      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "      word2idx[word] = idx\n",
    "      idx2word[idx] = word\n",
    "\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "\n",
    "    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    target_columns = df_submission.columns[1:]\n",
    "    y_train = df_train[target_columns]\n",
    "\n",
    "    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n",
    "    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n",
    "    inpan = Input(shape=(maxlen_an,),name='inpan')\n",
    "    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n",
    "    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n",
    "    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n",
    "    if(rnn_type==\"LSTM\"):\n",
    "        BLqt = Bidirectional(LSTM(rnn_units))(Eqt)\n",
    "        BLqb = Bidirectional(LSTM(rnn_units))(Eqb)\n",
    "        BLan = Bidirectional(LSTM(rnn_units))(Ean)\n",
    "    elif(rnn_type==\"GRU\"):\n",
    "        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n",
    "        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n",
    "        BLan = Bidirectional(GRU(rnn_units))(Ean)\n",
    "    Dqt = Dropout(dropout_rate)(BLqt)\n",
    "    Dqb = Dropout(dropout_rate)(BLqb)\n",
    "    Dan = Dropout(dropout_rate)(BLan)\n",
    "    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n",
    "    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n",
    "    Dsf = Dense(30, activation='sigmoid')(Ds)\n",
    "\n",
    "    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n",
    "\n",
    "    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n",
    "    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n",
    "    target_columns = df_submission.columns\n",
    "    outp = {}\n",
    "    outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "    for i in range(1,len(target_columns)):\n",
    "        outp[target_columns[i]] = y_test[:, i-1]\n",
    "    my_submission = pd.DataFrame(outp)\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "def LSTM_model_stacked(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n",
    "                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n",
    "                      dropout_rate=0.2, dense_hidden_units=60, num_stacks=2, epochs=2):\n",
    "    columns = ['question_title','question_body','answer']\n",
    "    df_train = clean_data(df_train, columns)\n",
    "    df_test = clean_data(df_test, columns)\n",
    "    # columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n",
    "      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "      word2idx[word] = idx\n",
    "      idx2word[idx] = word\n",
    "\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "\n",
    "    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    target_columns = df_submission.columns[1:]\n",
    "    y_train = df_train[target_columns]\n",
    "\n",
    "    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n",
    "    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n",
    "    inpan = Input(shape=(maxlen_an,),name='inpan')\n",
    "    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n",
    "    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n",
    "    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n",
    "    if(rnn_type==\"LSTM\"):\n",
    "        BLqt = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqt)\n",
    "        BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqb)\n",
    "        BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(Ean)\n",
    "        for i in range(num_stacks-1):\n",
    "            BLqt = Bidirectional(LSTM(rnn_units, return_sequences=True))(BLqt)\n",
    "            BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(BLqb)\n",
    "            BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(BLan)\n",
    "    elif(rnn_type==\"GRU\"):\n",
    "        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n",
    "        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n",
    "        BLan = Bidirectional(GRU(rnn_units))(Ean)\n",
    "    Dqt = Dropout(dropout_rate)(Lambda(lambda x: x[:,-1,:], output_shape=(128,))(BLqt))\n",
    "    Dqb = Dropout(dropout_rate)(Lambda(lambda x: x[:,-1,:], output_shape=(128,))(BLqb))\n",
    "    Dan = Dropout(dropout_rate)(Lambda(lambda x: x[:,-1,:], output_shape=(128,))(BLan))\n",
    "    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n",
    "    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n",
    "    Dsf = Dense(30, activation='sigmoid')(Ds)\n",
    "\n",
    "    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n",
    "\n",
    "    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n",
    "    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n",
    "    target_columns = df_submission.columns\n",
    "    outp = {}\n",
    "    outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "    for i in range(1,len(target_columns)):\n",
    "        outp[target_columns[i]] = y_test[:, i-1]\n",
    "    my_submission = pd.DataFrame(outp)\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "# from keras.layers import Lambda, Dot, Activation, Average\n",
    "\n",
    "def attention_3d_block_self(hidden_states, rnn_units=64):\n",
    "    hidden_size = int(hidden_states.shape[2])\n",
    "    score_first_part = Dense(hidden_size, use_bias=False)(hidden_states)\n",
    "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,))(hidden_states)\n",
    "    score = Dot([2, 1])([score_first_part, h_t])\n",
    "    attention_weights = Activation('softmax')(score)\n",
    "    context_vector = Dot([1, 1])([hidden_states, attention_weights])\n",
    "    pre_activation = Concatenate()([context_vector, h_t])\n",
    "    attention_vector = Dense(rnn_units*2, use_bias=False, activation='tanh')(pre_activation)\n",
    "    return attention_vector\n",
    "    \n",
    "    \n",
    "def attention_3d_block_another(hidden_states1, hidden_state2,rnn_units=64):\n",
    "    hidden_size = int(hidden_states1.shape[2])\n",
    "    score_first_part = Dense(hidden_size, use_bias=False)(hidden_states1)\n",
    "    score = Dot([2, 1])([score_first_part, hidden_state2])\n",
    "    attention_weights = Activation('softmax')(score)\n",
    "    context_vector = Dot([1, 1])([hidden_states1, attention_weights])\n",
    "    pre_activation = Concatenate()([context_vector, hidden_state2])\n",
    "    attention_vector = Dense(rnn_units*2, use_bias=False, activation='tanh')(pre_activation)\n",
    "    return attention_vector/kaggle/input/xlnet-base-tf/xlnet-base-cased-spiece.model\n",
    "\n",
    "def LSTM_model_modified_with_attention_self_with_lib(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n",
    "                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n",
    "                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n",
    "    columns = ['question_title','question_body','answer']\n",
    "    df_train = clean_data(df_train, columns)\n",
    "    df_test = clean_data(df_test, columns)\n",
    "    # columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n",
    "      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "      word2idx[word] = idx\n",
    "      idx2word[idx] = word\n",
    "\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "\n",
    "    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    target_columns = df_submission.columns[1:]\n",
    "    y_train = df_train[target_columns]\n",
    "\n",
    "    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n",
    "    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n",
    "    inpan = Input(shape=(maxlen_an,),name='inpan')\n",
    "    \n",
    "    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n",
    "    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n",
    "    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n",
    "    \n",
    "    if(rnn_type==\"LSTM\"):\n",
    "        BLqt = Bidirectional(LSTM(rnn_units, return_state=True))(Eqt)\n",
    "        BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqb, initial_state=BLqt[1:])\n",
    "        BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(Ean)\n",
    "    elif(rnn_type==\"GRU\"):\n",
    "        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n",
    "        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n",
    "        BLan = Bidirectional(GRU(rnn_units))(Ean)\n",
    "    \n",
    "    AtQ = SeqSelfAttention(attention_activation='sigmoid')(BLqb)\n",
    "    AtAn = SeqSelfAttention(attention_activation='sigmoid')(BLan)\n",
    "    \n",
    "    Mq = Average()([Lambda(lambda x: x[:,i,:], output_shape=(rnn_units*2,))(AtQ) for i in range(maxlen_qb)])\n",
    "    Man = Average()([Lambda(lambda x: x[:,i,:], output_shape=(rnn_units*2,))(AtAn) for i in range(maxlen_an)])\n",
    "    \n",
    "#     Dqb = Dropout(dropout_rate)(qbin)\n",
    "#     Dan = Dropout(dropout_rate)(anin)\n",
    "    \n",
    "    Concatenated = Concatenate()([Mq, Man])\n",
    "    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n",
    "    Dsf = Dense(30, activation='sigmoid')(Ds)\n",
    "\n",
    "    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=128, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n",
    "\n",
    "    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n",
    "    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n",
    "    target_columns = df_submission.columns\n",
    "    outp = {}\n",
    "    outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "    for i in range(1,len(target_columns)):\n",
    "        outp[target_columns[i]] = y_test[:, i-1]\n",
    "    my_submission = pd.DataFrame(outp)\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "def LSTM_model_modified_with_attention_self(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n",
    "                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n",
    "                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n",
    "    columns = ['question_title','question_body','answer']\n",
    "    df_train = clean_data(df_train, columns)\n",
    "    df_test = clean_data(df_test, columns)\n",
    "    # columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n",
    "      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "      word2idx[word] = idx\n",
    "      idx2word[idx] = word\n",
    "\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "\n",
    "    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    target_columns = df_submission.columns[1:]\n",
    "    y_train = df_train[target_columns]\n",
    "\n",
    "    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n",
    "    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n",
    "    inpan = Input(shape=(maxlen_an,),name='inpan')\n",
    "    \n",
    "    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n",
    "    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n",
    "    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n",
    "    \n",
    "    if(rnn_type==\"LSTM\"):\n",
    "        BLqt_out, BLqt_h1, BLqt_c1, BLqt_h2, BLqt_c2 = Bidirectional(LSTM(rnn_units, return_state=True, return_sequences=True))(Eqt)\n",
    "        BLqb_out, BLqb_h1, BLqb_c1, BLqb_h2, BLqb_c2 = Bidirectional(LSTM(rnn_units, return_state=True, return_sequences=True))(Eqb, initial_state=[BLqt_h1, BLqt_c1, BLqt_h2, BLqt_c2])\n",
    "        BLan_out, BLan_h1, BLan_c1, BLan_h2, BLan_c2 = Bidirectional(LSTM(rnn_units, return_state=True, return_sequences=True))(Ean)\n",
    "    elif(rnn_type==\"GRU\"):\n",
    "        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n",
    "        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n",
    "        BLan = Bidirectional(GRU(rnn_units))(Ean)\n",
    "    \n",
    "#     AtQ = attention_3d_block_self(BLqb, rnn_units)\n",
    "#     AtAn = attention_3d_block_self(BLan, rnn_units)\n",
    "#     Dqt = Dropout(dropout_rate)(BLqt[0])\n",
    "    qbin = Lambda(lambda x: x[:,-1,:], output_shape=(rnn_units*2,), name=\"lambda_layer1\")(BLqb_out)\n",
    "    anin = Lambda(lambda x: x[:,-1,:], output_shape=(rnn_units*2,), name=\"lambda_layer2\")(BLan_out)\n",
    "    attn_out, attn_states = AttentionLayer()([BLqb_out, BLan_out], verbose=True)\n",
    "    Dqb = Dropout(dropout_rate)(qbin)\n",
    "    Dan = Dropout(dropout_rate)(anin)\n",
    "    print(Dqb.shape, Dan.shape, attn_out.shape)\n",
    "    Concatenated = Concatenate()([Dqb, Dan, attn_out])\n",
    "    Ds = Dense(dense_hidden_units, activation='elu')(Concatenated)\n",
    "    Dsf = Dense(30, activation='sigmoid')(Ds)\n",
    "\n",
    "    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n",
    "\n",
    "    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n",
    "    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n",
    "    target_columns = df_submission.columns\n",
    "    outp = {}\n",
    "    outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "    for i in range(1,len(target_columns)):\n",
    "        outp[target_columns[i]] = y_test[:, i-1]\n",
    "    my_submission = pd.DataFrame(outp)\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "def LSTM_model_modified_with_attention_a2q(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n",
    "                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n",
    "                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n",
    "    columns = ['question_title','question_body','answer']\n",
    "    df_train = clean_data(df_train, columns)\n",
    "    df_test = clean_data(df_test, columns)\n",
    "    # columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n",
    "      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "      word2idx[word] = idx\n",
    "      idx2word[idx] = word\n",
    "\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "\n",
    "    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    target_columns = df_submission.columns[1:]\n",
    "    y_train = df_train[target_columns]\n",
    "\n",
    "    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n",
    "    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n",
    "    inpan = Input(shape=(maxlen_an,),name='inpan')\n",
    "    \n",
    "    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n",
    "    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n",
    "    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n",
    "    \n",
    "    if(rnn_type==\"LSTM\"):\n",
    "        BLqt = Bidirectional(LSTM(rnn_units, return_state=True))(Eqt)\n",
    "        BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqb, initial_state=BLqt[1:])\n",
    "        BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(Ean)\n",
    "    elif(rnn_type==\"GRU\"):\n",
    "        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n",
    "        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n",
    "        BLan = Bidirectional(GRU(rnn_units))(Ean)\n",
    "    \n",
    "    AtA2Q = Average()([attention_3d_block_another(BLqb, Lambda(lambda x: x[:,i,:], output_shape=(rnn_units*2,))(BLan), rnn_units) for i in range(maxlen_an)])\n",
    "#     Dqt = Dropout(dropout_rate)(BLqt[0])\n",
    "    Dqbin = Lambda(lambda x: x[:,-1,:], output_shape=(rnn_units*2,), name=\"lambda_layer1\")(BLqb)\n",
    "    Dqb = Dropout(dropout_rate)(Dqbin)\n",
    "    Danin = Lambda(lambda x: x[:,-1,:], output_shape=(rnn_units*2,), name=\"lambda_layer2\")(BLan)\n",
    "    Dan = Dropout(dropout_rate)(Danin)\n",
    "    \n",
    "    Concatenated = Concatenate()([Dqb, Dan, AtA2Q])\n",
    "    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n",
    "    Dsf = Dense(30, activation='sigmoid')(Ds)\n",
    "\n",
    "    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n",
    "\n",
    "    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n",
    "    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n",
    "    target_columns = df_submission.columns\n",
    "    outp = {}\n",
    "    outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "    for i in range(1,len(target_columns)):\n",
    "        outp[target_columns[i]] = y_test[:, i-1]\n",
    "    my_submission = pd.DataFrame(outp)\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    \n",
    "def LSTM_model_modified_concatenated_qa(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n",
    "                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n",
    "                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n",
    "    columns = ['question_title','question_body','answer']\n",
    "    df_train = clean_data(df_train, columns)\n",
    "    df_test = clean_data(df_test, columns)\n",
    "    # columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n",
    "      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "      word2idx[word] = idx\n",
    "      idx2word[idx] = word\n",
    "\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "\n",
    "    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    target_columns = df_submission.columns[1:]\n",
    "    y_train = df_train[target_columns]\n",
    "\n",
    "    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n",
    "    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n",
    "    inpan = Input(shape=(maxlen_an,),name='inpan')\n",
    "    \n",
    "    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n",
    "    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n",
    "    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n",
    "    \n",
    "    if(rnn_type==\"LSTM\"):\n",
    "        BLqt = Bidirectional(LSTM(rnn_units, return_state=True))(Eqt)\n",
    "        BLqb = Bidirectional(LSTM(rnn_units, return_state=True))(Eqb, initial_state=BLqt[1:])\n",
    "        BLan = Bidirectional(LSTM(rnn_units, return_state=True))(Ean, initial_state=BLqb[1:])\n",
    "    elif(rnn_type==\"GRU\"):\n",
    "        BLqt = Bidirectional(GRU(rnn_units, return_state=True))(Eqt)\n",
    "        BLqb = Bidirectional(GRU(rnn_units, return_state=True))(Eqb, initial_state=BLqt[1:])\n",
    "        BLan = Bidirectional(GRU(rnn_units, return_state=True))(Ean, initial_state=BLqb[1:])\n",
    "        \n",
    "    Dqt = Dropout(dropout_rate)(BLqt[0])\n",
    "    Dqb = Dropout(dropout_rate)(BLqb[0])\n",
    "    Dan = Dropout(dropout_rate)(BLan[0])\n",
    "    \n",
    "    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n",
    "    \n",
    "    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n",
    "    Dsf = Dense(30, activation='sigmoid')(Ds)\n",
    "\n",
    "    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n",
    "\n",
    "    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n",
    "    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n",
    "    target_columns = df_submission.columns\n",
    "    outp = {}\n",
    "    outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "    for i in range(1,len(target_columns)):\n",
    "        outp[target_columns[i]] = y_test[:, i-1]\n",
    "    my_submission = pd.DataFrame(outp)\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "# LSTM_model_stacked(df_train, df_test, df_submission, rnn_type=\"LSTM\")\n",
    "# LSTM_model_modified_with_attention_self_with_lib(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n",
    "#                        rnn_units=64, maxlen_qt = 40, maxlen_qb = 260, maxlen_an = 210,\n",
    "#                       dropout_rate=0.2, dense_hidden_units=40, epochs=3)\n",
    "# y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dan_model(df_train, df_test, df_submission, batch_size=8, epochs=4, hidden_layers=[120]):\n",
    "  if(len(hidden_layers)<1):\n",
    "    print(\"Non-Empty Hidden Layers List Required!\")\n",
    "    return\n",
    "  module_url = \"/kaggle/input/sent-embed-model\"\n",
    "  model = hub.load(module_url)\n",
    "\n",
    "  def embed(input):\n",
    "    return model(input)\n",
    "\n",
    "  qt_train = np.array(embed(df_train[\"question_title\"]))\n",
    "  qb_train = np.array(embed(df_train[\"question_body\"]))\n",
    "  an_train = np.array(embed(df_train[\"answer\"]))\n",
    "\n",
    "  X_train = np.concatenate([qt_train, qb_train, an_train], axis=1)\n",
    "\n",
    "  qt_test = np.array(embed(df_test[\"question_title\"]))\n",
    "  qb_test = np.array(embed(df_test[\"question_body\"]))\n",
    "  an_test = np.array(embed(df_test[\"answer\"]))\n",
    "\n",
    "  X_test = np.concatenate([qt_test, qb_test, an_test], axis=1)\n",
    "\n",
    "  target_columns = df_submission.columns[1:]\n",
    "  y_train = df_train[target_columns].values\n",
    "\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(hidden_layers[0], activation=\"relu\"))\n",
    "  model.add(tf.keras.layers.Dropout(0.2))  \n",
    "  for h in hidden_layers[1:]:\n",
    "    model.add(tf.keras.layers.Dense(h, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))  \n",
    "  model.add(tf.keras.layers.Dense(30, activation=\"sigmoid\"))\n",
    "\n",
    "  model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "  model.fit(X_train, y_train, batch_size=batch_size,\n",
    "            epochs=epochs, validation_split=0.1)\n",
    "  print(model.summary())\n",
    "  y_test = model.predict(X_test)\n",
    "\n",
    "  outp = {}\n",
    "  outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "  for i in range(len(target_columns)):\n",
    "      outp[target_columns[i]] = y_test[:, i]\n",
    "  my_submission = pd.DataFrame(outp)\n",
    "  my_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "def pad_seq_custom(original_series, maxlen=50):\n",
    "    n = original_series.size\n",
    "    for i in range(n):\n",
    "        ll = len(original_series[i])\n",
    "        for kk in range(maxlen-ll):\n",
    "            np.append(original_series[i],[0 for rr in range(512)])\n",
    "    return original_series\n",
    "    \n",
    "    \n",
    "def dan_model_with_words(df_train, df_test, df_submission, batch_size=8,\n",
    "                         epochs=4, hidden_layers=[120],\n",
    "                         rnn_units=64, maxlen_qt = 26,\n",
    "                         maxlen_qb = 260, maxlen_an = 210,\n",
    "                         dropout_rate=0.2, dense_hidden_units=50,\n",
    "                         embedding_size=512):\n",
    "    if(len(hidden_layers)<1):\n",
    "        print(\"Non-Empty Hidden Layers List Required!\")\n",
    "        return\n",
    "    module_url = \"/kaggle/input/sent-embed-model\"\n",
    "    model = hub.load(module_url)\n",
    "\n",
    "    def embed(input):\n",
    "        return model([input])\n",
    "\n",
    "    columns = ['question_title','question_body','answer']\n",
    "    df_train = clean_data(df_train, columns)\n",
    "    df_test = clean_data(df_test, columns)\n",
    "    # columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n",
    "      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "      word2idx[word] = idx\n",
    "      idx2word[idx] = word\n",
    "\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "\n",
    "    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    target_columns = df_submission.columns[1:]\n",
    "    y_train = df_train[target_columns]\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size+1, embedding_size))\n",
    "    for word, i in word2idx.items():\n",
    "        embedding = np.array(embed(word))\n",
    "        if embedding is not None:\n",
    "            embedding_matrix[i] = embedding\n",
    "\n",
    "\n",
    "    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n",
    "    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n",
    "    inpan = Input(shape=(maxlen_an,),name='inpan')\n",
    "    \n",
    "    Eqt = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_qt, trainable=False)(inpqt)\n",
    "    Eqb = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_qb, trainable=False)(inpqb)\n",
    "    Ean = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_an, trainable=False)(inpan)\n",
    "\n",
    "    BLqt = Bidirectional(LSTM(rnn_units))(Eqt)\n",
    "    BLqb = Bidirectional(LSTM(rnn_units))(Eqb)\n",
    "    BLan = Bidirectional(LSTM(rnn_units))(Ean)\n",
    "    \n",
    "    Dqt = Dropout(dropout_rate)(BLqt)\n",
    "    Dqb = Dropout(dropout_rate)(BLqb)\n",
    "    Dan = Dropout(dropout_rate)(BLan)\n",
    "\n",
    "    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n",
    "    \n",
    "    Ds = Dense(dense_hidden_units, activation='elu')(Concatenated)\n",
    "    Dsf = Dense(30, activation='sigmoid')(Ds)\n",
    "\n",
    "    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=8, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n",
    "\n",
    "    outp = {}\n",
    "    outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "    for i in range(len(target_columns)):\n",
    "        outp[target_columns[i]] = y_test[:, i]\n",
    "    my_submission = pd.DataFrame(outp)\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "# dan_model_with_words(df_train, df_test, df_submission, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# df_train = pd.read_csv(\"/kaggle/input/google-quest-challenge/train.csv\")\n",
    "# df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n",
    "# df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n",
    "\n",
    "def glove_model_with_words(df_train, df_test, df_submission, batch_size=8,\n",
    "                         epochs=4, rnn_units=64, maxlen_qt = 26,\n",
    "                         maxlen_qb = 260, maxlen_an = 210,\n",
    "                         dropout_rate=0.2, dense_hidden_units=50,\n",
    "                         embedding_size=300):\n",
    "    filename = '/kaggle/input/googlenewsvectors/GoogleNews-vectors-negative300.bin'\n",
    "    model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "    def embed(model, input):\n",
    "        if input not in model.wv.vocab:\n",
    "            return np.array([0 for _ in range(300)])\n",
    "        return model.wv[input]\n",
    "\n",
    "    columns = ['question_title','question_body','answer']\n",
    "    df_train = clean_data(df_train, columns)\n",
    "    df_test = clean_data(df_test, columns)\n",
    "    # columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n",
    "      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n",
    "    vocab = sorted(list(set(tokens)))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for idx, word in enumerate(vocab):\n",
    "      word2idx[word] = idx\n",
    "      idx2word[idx] = word\n",
    "\n",
    "    for col in columns:\n",
    "      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n",
    "\n",
    "    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n",
    "    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n",
    "    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n",
    "\n",
    "    target_columns = df_submission.columns[1:]\n",
    "    y_train = df_train[target_columns]\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size+1, embedding_size))\n",
    "    for word, i in word2idx.items():\n",
    "        embedding = np.array(embed(model, word))\n",
    "        if embedding is not None:\n",
    "            embedding_matrix[i] = embedding\n",
    "\n",
    "\n",
    "    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n",
    "    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n",
    "    inpan = Input(shape=(maxlen_an,),name='inpan')\n",
    "    \n",
    "    Eqt = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_qt)(inpqt)\n",
    "    Eqb = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_qb)(inpqb)\n",
    "    Ean = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_an)(inpan)\n",
    "\n",
    "    BLqt = Bidirectional(LSTM(rnn_units))(Eqt)\n",
    "    BLqb = Bidirectional(LSTM(rnn_units))(Eqb)\n",
    "    BLan = Bidirectional(LSTM(rnn_units))(Ean)\n",
    "\n",
    "    Dqt = Dropout(dropout_rate)(BLqt)\n",
    "    Dqb = Dropout(dropout_rate)(BLqb)\n",
    "    Dan = Dropout(dropout_rate)(BLan)\n",
    "    \n",
    "    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n",
    "    \n",
    "    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n",
    "    Dsf = Dense(30, activation='sigmoid')(Ds)\n",
    "\n",
    "    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n",
    "\n",
    "    outp = {}\n",
    "    outp[\"qa_id\"] = df_test[\"qa_id\"]\n",
    "    for i in range(len(target_columns)):\n",
    "        outp[target_columns[i]] = y_test[:, i]\n",
    "    my_submission = pd.DataFrame(outp)\n",
    "    my_submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "# glove_model_with_words(df_train, df_test, df_submission, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.keras.backend as K\n",
    "# import os\n",
    "# from scipy.stats import spearmanr\n",
    "# from math import floor, ceil\n",
    "# from transformers import *\n",
    "\n",
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, length, is_pair = True):\n",
    "        if is_pair:\n",
    "            inputs = tokenizer.encode_plus(str1, str2,\n",
    "                add_special_tokens=True,\n",
    "                max_length=length,\n",
    "                truncation_strategy=\"only_second\")\n",
    "        else:\n",
    "            inputs = tokenizer.encode_plus(str1, None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=length,\n",
    "                truncation_strategy=\"only_first\")\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title, question, max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        answer, None, max_sequence_length, False)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]\n",
    "\n",
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    \n",
    "    config = BertConfig() # print(config) to see settings\n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "    bert_model = TFBertModel.from_pretrained(\n",
    "        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def bert_uncased():\n",
    "    PATH = '/kaggle/input/google-quest-challenge/'\n",
    "\n",
    "    BERT_PATH = '/kaggle/input/bert-base-uncased-huggingface-transformer/'\n",
    "    # !cp /kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-vocab.txt ./vocab.txt\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_PATH + \"bert-base-uncased-vocab.txt\")\n",
    "\n",
    "    MAX_LEN = 90\n",
    "\n",
    "    df_train = pd.read_csv(PATH+'train.csv')\n",
    "    df_test = pd.read_csv(PATH+'test.csv')\n",
    "    df_submission = pd.read_csv(PATH+'sample_submission.csv')\n",
    "\n",
    "    output_categories = list(df_submission.columns)\n",
    "    input_categories = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    \n",
    "    outputs = df_train[output_categories[1:]].values\n",
    "    inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_LEN)\n",
    "    test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_LEN)\n",
    "    \n",
    "    test_preds = []\n",
    "    train_inputs = [inputs[i] for i in range(len(inputs))]\n",
    "    train_outputs = outputs\n",
    "\n",
    "    K.clear_session()\n",
    "    model = create_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "    logdir = \"/kaggle/working/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    model.fit(train_inputs, train_outputs, epochs=1, batch_size=16, validation_split=0.5, callbacks=[tensorboard_callback])\n",
    "\n",
    "    test_preds.append(model.predict(test_inputs))\n",
    "    df_submission.iloc[:, 1:] = np.average(test_preds, axis=0)\n",
    "    df_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5471 samples, validate on 608 samples\n",
      "Epoch 1/5\n",
      "5471/5471 [==============================] - 1043s 191ms/sample - loss: 0.4118 - val_loss: 0.3789\n",
      "Epoch 2/5\n",
      "5471/5471 [==============================] - 1007s 184ms/sample - loss: 0.3726 - val_loss: 0.3669\n",
      "Epoch 3/5\n",
      "5471/5471 [==============================] - 1006s 184ms/sample - loss: 0.3609 - val_loss: 0.3660\n",
      "Epoch 4/5\n",
      "5471/5471 [==============================] - 1006s 184ms/sample - loss: 0.3512 - val_loss: 0.3695\n",
      "Epoch 5/5\n",
      "5471/5471 [==============================] - 1005s 184ms/sample - loss: 0.3410 - val_loss: 0.3698\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "\n",
    "PATH = '/content/drive/My Drive/NLPDATA/'\n",
    "BERT_PATH = '/kaggle/input/bert-base-uncased-huggingface-transformer/'\n",
    "MAX_LEN = 360\n",
    "\n",
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, length, is_pair = True):\n",
    "        if is_pair:\n",
    "            inputs = tokenizer.encode_plus(str1, str2,\n",
    "                add_special_tokens=True,\n",
    "                max_length=length,\n",
    "                truncation_strategy=\"only_second\")\n",
    "        else:\n",
    "            inputs = tokenizer.encode_plus(str1, None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=length,\n",
    "                truncation_strategy=\"only_first\")\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title, question, max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        answer, None, max_sequence_length, False)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in df[columns].iterrows():\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]\n",
    "\n",
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    \n",
    "    config = XLNetConfig()\n",
    "    config.d_inner = 3072\n",
    "    config.n_head = 12\n",
    "    config.d_model = 768\n",
    "    config.n_layer = 12\n",
    "    config.output_hidden_states = False\n",
    "    \n",
    "    bert_model = TFXLNetModel.from_pretrained(\n",
    "        '/kaggle/input/xlnet-base-tf/xlnet-base-cased-tf_model.h5', config=config)\n",
    "    \n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def xlnet_cased():\n",
    "    PATH = '/kaggle/input/google-quest-challenge/'\n",
    "\n",
    "    BERT_PATH = '/kaggle/input/xlnet-base-tf/'\n",
    "    # !cp /kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-vocab.txt ./vocab.txt\n",
    "#     tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "    tokenizer = XLNetTokenizer(\"/kaggle/input/xlnet-base-tf/xlnet-base-cased-spiece.model\")\n",
    "\n",
    "    df_train = pd.read_csv(PATH+'train.csv')\n",
    "    df_test = pd.read_csv(PATH+'test.csv')\n",
    "    df_submission = pd.read_csv(PATH+'sample_submission.csv')\n",
    "\n",
    "    output_categories = list(df_submission.columns)\n",
    "    input_categories = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    \n",
    "    outputs = df_train[output_categories[1:]].values\n",
    "    inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_LEN)\n",
    "    test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_LEN)\n",
    "    \n",
    "    test_preds = []\n",
    "    train_inputs = [inputs[i] for i in range(len(inputs))]\n",
    "    train_outputs = outputs\n",
    "\n",
    "    K.clear_session()\n",
    "    model = create_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    model.fit(train_inputs, train_outputs, epochs=5, batch_size=6, validation_split=0.1)\n",
    "\n",
    "    test_preds.append(model.predict(test_inputs))\n",
    "    df_submission.iloc[:, 1:] = np.average(test_preds, axis=0)\n",
    "    df_submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "xlnet_cased()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
